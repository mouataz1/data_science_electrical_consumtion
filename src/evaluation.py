"""
Module d'√©valuation des mod√®les
"""

import numpy as np
from sklearn.metrics import (
    mean_squared_error, mean_absolute_error, r2_score,
    mean_absolute_percentage_error
)
import pandas as pd


class ModelEvaluator:
    """
    Classe pour √©valuer les mod√®les de r√©gression
    """

    @staticmethod
    def evaluate_regression(y_true, y_pred, model_name="Model"):
        """
        √âvalue un mod√®le de r√©gression

        Args:
            y_true (array): Valeurs r√©elles
            y_pred (array): Pr√©dictions
            model_name (str): Nom du mod√®le

        Returns:
            dict: M√©triques d'√©valuation
        """
        r2 = r2_score(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        mae = mean_absolute_error(y_true, y_pred)
        mape = mean_absolute_percentage_error(y_true, y_pred) * 100

        print(f"\n{'='*60}")
        print(f"√âVALUATION: {model_name}")
        print(f"{'='*60}")
        print(f"R¬≤ Score:     {r2:.4f}")
        print(f"RMSE:         {rmse:.4f}")
        print(f"MAE:          {mae:.4f}")
        print(f"MAPE:         {mape:.2f}%")
        print(f"{'='*60}")

        return {
            'Model': model_name,
            'R¬≤': r2,
            'RMSE': rmse,
            'MAE': mae,
            'MAPE (%)': mape
        }

    @staticmethod
    def compare_models(results):
        """
        Compare plusieurs mod√®les

        Args:
            results (list): Liste de dictionnaires de r√©sultats

        Returns:
            pd.DataFrame: Tableau de comparaison
        """
        df = pd.DataFrame(results)
        print("\n" + "="*80)
        print("COMPARAISON DES MOD√àLES")
        print("="*80)
        print(df.to_string(index=False))
        print("="*80)

        # Identifier le meilleur mod√®le
        best_idx = df['R¬≤'].idxmax()
        best_model = df.loc[best_idx, 'Model']
        best_r2 = df.loc[best_idx, 'R¬≤']

        print(f"\nüèÜ MEILLEUR MOD√àLE: {best_model}")
        print(f"   R¬≤ = {best_r2:.4f}\n")

        return df

    @staticmethod
    def calculate_residuals(y_true, y_pred):
        """
        Calcule les r√©sidus

        Args:
            y_true (array): Valeurs r√©elles
            y_pred (array): Pr√©dictions

        Returns:
            array: R√©sidus (y_true - y_pred)
        """
        return y_true - y_pred

    @staticmethod
    def check_overfitting(train_score, test_score, threshold=0.1):
        """
        V√©rifie le surapprentissage

        Args:
            train_score (float): Score sur train
            test_score (float): Score sur test
            threshold (float): Seuil de diff√©rence acceptable

        Returns:
            dict: Diagnostic
        """
        diff = train_score - test_score
        is_overfitting = diff > threshold

        print(f"\n{'='*60}")
        print("DIAGNOSTIC OVERFITTING")
        print(f"{'='*60}")
        print(f"Score train: {train_score:.4f}")
        print(f"Score test:  {test_score:.4f}")
        print(f"Diff√©rence:  {diff:.4f}")

        if is_overfitting:
            print("‚ö† ATTENTION: Surapprentissage d√©tect√©!")
            print("   Le mod√®le performe mieux sur train que sur test")
        else:
            print("‚úì Pas de surapprentissage significatif")

        print(f"{'='*60}\n")

        return {
            'train_score': train_score,
            'test_score': test_score,
            'difference': diff,
            'is_overfitting': is_overfitting
        }
